{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Some organisations have already built applications that work with OpenAI compatible API and would like to switch to Amazon Bedrock -- this guide is to help you achieve that without changing app code by using LiteLLM.</p>"},{"location":"99-contributors/","title":"Contributors (sorted alphabetically)","text":"<ul> <li>Amit Lulla</li> <li>Islam Mahgoub</li> <li>Naresh Nagpal</li> <li>Sam Sanders</li> </ul>"},{"location":"10-architecture/10-architecture/","title":"Architecture","text":"<p>The diagram below depicts the solution architecture. LiteLLM is used as a proxy to translate the API call originating from the app in OpenAI format to Bedrock format.</p> <p></p> <p>LiteLLM is deployed on Amazon EKS. If the app is hosted on the same cluster, it can access LiteLLM internally through Kubernetes <code>Service</code> of <code>type</code> <code>ClusterIP</code>. If the app is hosted outside the cluster, LiteLLM has to be exposed via a load balancer -- refer to Exposing applications section of Amazon EKS workshop for guidance. This implementation assumes the app is hosted on the same cluster.</p> <p>While LiteLLM is only used as a proxy in this implementation, it has several other features e.g. retry/fallback logic across multiple deployments, track spend &amp; set budgets per project, etc.</p>"},{"location":"20-deploy/20-configure-env-variables/","title":"Configure environment variables","text":"<p>Note</p> <p>The steps in the following sections have been tested on Cloud9/Amazon Linux. Make sure to disable AWS managed temporary credentials and attach an IAM role with sufficient permissions.</p> <ol> <li>Configure environment variables</li> </ol> <pre><code>export TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nexport AWS_REGION=`curl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/placement/region`\nexport CLUSTER_NAME=\"litellm-demo\"\n\necho \"export AWS_REGION=${AWS_REGION}\" | tee -a ~/.bash_profile\necho \"export CLUSTER_NAME=${CLUSTER_NAME}\" | tee -a ~/.bash_profile\n</code></pre>"},{"location":"20-deploy/25-clone-repo/","title":"Clone the repo","text":"<ol> <li>Clone bedrock-litellm repo <pre><code>git clone https://github.com/aws-samples/bedrock-litellm.git\n</code></pre></li> <li>Save bedrock-litellm directory in an environment variable <pre><code>export BEDROCK_LITELLM_DIR=$PWD/bedrock-litellm\necho \"export BEDROCK_LITELLM_DIR=${BEDROCK_LITELLM_DIR}\" | tee -a ~/.bash_profile\n</code></pre></li> </ol>"},{"location":"20-deploy/30-tools/","title":"Install Kubernetes tools","text":"<ol> <li> <p>Install eksctl: <pre><code># for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`\nARCH=amd64\nPLATFORM=$(uname -s)_$ARCH\n\ncurl -sLO \"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\"\n\n# (Optional) Verify checksum\ncurl -sL \"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt\" | grep $PLATFORM | sha256sum --check\n\ntar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz\n\nsudo mv /tmp/eksctl /usr/local/bin\n</code></pre></p> </li> <li> <p>Install kubectl: <pre><code>curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.30.0/2024-05-12/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nmkdir -p $HOME/bin &amp;&amp; cp ./kubectl $HOME/bin/kubectl &amp;&amp; export PATH=$HOME/bin:$PATH\n</code></pre></p> </li> <li> <p>Install yq: <pre><code>echo 'yq() {\n  docker run --rm -i -v \"${PWD}\":/workdir mikefarah/yq \"$@\"\n}' | tee -a ~/.bashrc &amp;&amp; source ~/.bashrc\n</code></pre></p> </li> <li> <p>Install Helm: <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> </li> <li> <p>Install envsubst: <pre><code>curl -L https://github.com/a8m/envsubst/releases/download/v1.2.0/envsubst-`uname -s`-`uname -m` -o envsubst\nchmod +x envsubst\nsudo mv envsubst /usr/local/bin\n</code></pre></p> </li> </ol>"},{"location":"20-deploy/40-create-prepare-cluster/","title":"Create and prepare an EKS cluster","text":"<ol> <li> <p>Create cluster: <pre><code>envsubst &lt; $BEDROCK_LITELLM_DIR/eksctl/cluster-config.yaml | eksctl create cluster -f -\n</code></pre></p> </li> <li> <p>Create an IAM OIDC provider for the cluster to be able to use IAM roles for service accounts (required for granting IAM permissions to LiteLLM to be able to invoke Bedrock models): <pre><code>eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve\n</code></pre></p> </li> <li> <p>(Optional) Install AWS Load Balancer Controller (AWS LBC):</p> <p>Note</p> <p>Install AWS Load Balancer Controller if you are planning to expose LiteLLM or one of the clients on ELB.</p> <p>First, create the IAM policy:</p> <pre><code>curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json\n\nexport AWS_LBC_IAM_POLICY_ARN=$(aws iam create-policy \\\n--policy-name AWSLoadBalancerControllerIAMPolicy \\\n--policy-document file://iam_policy.json \\\n--output text \\\n--query \"Policy.Arn\")\necho \"export AWS_LBC_IAM_POLICY_ARN=${AWS_LBC_IAM_POLICY_ARN}\" | tee -a ~/.bash_profile\n</code></pre> <p>Then, create IRSA setup: <pre><code>eksctl create iamserviceaccount \\\n    --cluster $CLUSTER_NAME \\\n    --namespace=kube-system \\\n    --name=aws-load-balancer-controller \\\n    --role-name AmazonEKS_LoadBalancerController_Role \\\n    --attach-policy-arn $AWS_LBC_IAM_POLICY_ARN \\\n    --approve\n</code></pre> Then, install AWS LBC helm chart: <pre><code>helm repo add eks https://aws.github.io/eks-charts\nhelm repo update eks\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller \\\n--namespace kube-system \\\n--set clusterName=$CLUSTER_NAME \\\n--set serviceAccount.create=false \\\n--set serviceAccount.name=aws-load-balancer-controller \n</code></pre></p> </li> <li> <p>(Optional) Install EBS CSI driver (EBS volumes will be used to store Open WebUI state):</p> <p>Note</p> <p>Install EBS CSI driver if you are planning to use Open WebUI as it depends on EBS volumes for storing its state.</p> <p>First, create IRSA dependencies: <pre><code>eksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --namespace kube-system \\\n    --cluster $CLUSTER_NAME \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\n</code></pre> Then, install EBS CSI driver helm chart: <pre><code>helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver\nhelm repo update\nhelm upgrade --install aws-ebs-csi-driver \\\n    --namespace kube-system \\\n    --set controller.serviceAccount.create=false \\\n    aws-ebs-csi-driver/aws-ebs-csi-driver\n</code></pre></p> </li> </ol>"},{"location":"20-deploy/50-install-litellm/","title":"Install LiteLLM","text":"<ol> <li> <p>Clone LiteLLM repo <pre><code>git clone https://github.com/BerriAI/litellm.git\n</code></pre></p> </li> <li> <p>Save LiteLLM directory in an environment variable <pre><code>export LITELLM_DIR=$PWD/litellm\necho \"export LITELLM_DIR=${LITELLM_DIR}\" | tee -a ~/.bash_profile\n</code></pre></p> </li> <li> <p>Create IRSA dependencies for LiteLLM</p> <p>First, create IAM policy: <pre><code>envsubst &lt; $BEDROCK_LITELLM_DIR/iam/litellm-bedrock-policy.json &gt; /tmp/litellm-bedrock-policy.json\n\nexport LITELLM_BEDROCK_IAM_POLICY_ARN=$(aws iam create-policy \\\n--policy-name litellm-bedrock-policy \\\n--policy-document file:///tmp/litellm-bedrock-policy.json \\\n--output text \\\n--query \"Policy.Arn\")\necho \"export LITELLM_BEDROCK_IAM_POLICY_ARN=${LITELLM_BEDROCK_IAM_POLICY_ARN}\" | tee -a ~/.bash_profile\n</code></pre></p> <p>Then, create IRSA setup: <pre><code>eksctl create iamserviceaccount \\\n    --name litellm-sa \\\n    --cluster $CLUSTER_NAME \\\n    --role-name AmazonEKS_LiteLLM_Role \\\n    --attach-policy-arn $LITELLM_BEDROCK_IAM_POLICY_ARN \\\n    --approve\n</code></pre></p> </li> <li> <p>Install LiteLLM</p> <p>Note</p> <p>LiteLLM helm chart is currently BETA, hence K8s manifests were used for installation. The snippet below will be changed once the helm chart is GAed.</p> <p>Note</p> <p>Make sure to change <code>LITELLM_MASTER_KEY</code> in <code>$LITELLM_DIR/deploy/kubernetes/kub.yaml</code> to a random string rather than using the default API key, specially if you will expose LiteLLM endpoint externally.</p> <pre><code>yq -i '.spec.template.spec.serviceAccount= \"litellm-sa\"' $LITELLM_DIR/deploy/kubernetes/kub.yaml\nyq -i 'del(.spec.template.spec.containers[0].env[] | select(.name == \"DATABASE_URL\") )' $LITELLM_DIR/deploy/kubernetes/kub.yaml\nyq -i '.spec.type= \"ClusterIP\"' $LITELLM_DIR/deploy/kubernetes/service.yaml\n\nkubectl create configmap litellm-config --from-file=$BEDROCK_LITELLM_DIR/litellm/config/proxy_config.yaml\nkubectl apply -f $LITELLM_DIR/deploy/kubernetes/kub.yaml\nkubectl apply -f $LITELLM_DIR/deploy/kubernetes/service.yaml\n</code></pre> </li> <li> <p>Allow acess to Bedrock models by following the steps in this doc page.</p> </li> <li> <p>Ensure that LiteLLM pods are up and running</p> </li> <li> <p>Verify LiteLLM</p> <pre><code>kubectl run curl --image=curlimages/curl --rm -it -- /bin/sh\ncurl --location \"http://litellm-service.default.svc.cluster.local:4000/chat/completions\" \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer sk-1234' \\\n    --data '{\n    \"model\": \"bedrock-llama3-8b-instruct-v1\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"what llm are you\"\n        }\n    ]\n}'\n</code></pre> </li> </ol>"},{"location":"20-deploy/60-expose-litellm/","title":"(Optional) Expose LiteLLM","text":""},{"location":"20-deploy/60-expose-litellm/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A domain that can be used for hosting LiteLLM and exposing it externally through public endpoint.</li> <li>A digital certificate in AWS Certificate Manager (ACM) for enabling TLS on LiteLLM</li> </ul>"},{"location":"20-deploy/60-expose-litellm/#expose-litellm","title":"Expose LiteLLM","text":"<ol> <li> <p>Configure environment variables; replace <code>&lt;litellm-hostname&gt;</code>, <code>&lt;litellm-cert-arn&gt;</code> with the corresponding hostnames and ACM certificates ARN.     <pre><code>export LITELLM_HOSTNAME=\"&lt;litellm-hostname&gt;\"\nexport LITELLM_CERTIFICATE_ARN=\"&lt;litellm-cert-arn&gt;\"\n\necho \"export LITELLM_HOSTNAME=${LITELLM_HOSTNAME}\" | tee -a ~/.bash_profile\necho \"export LITELLM_CERTIFICATE_ARN=${LITELLM_CERTIFICATE_ARN}\" | tee -a ~/.bash_profile\n</code></pre></p> </li> <li> <p>Apply LiteLLM ingress     <pre><code>envsubst &lt; $BEDROCK_LITELLM_DIR/litellm/deploy/ingress.yaml | kubectl apply -f -\n</code></pre></p> <p>Note</p> <p>ELB needs a minute or so to complete the target registration; if the URL above did not work for you, wait for a few seconds for the registration to get completed.</p> </li> <li> <p>Extract LiteLLM URL:     <pre><code>kubectl get ingress litellm-ingress  -o jsonpath='{.status.loadBalancer.ingress[*].hostname}'\n</code></pre></p> </li> <li> <p>Add a CNAME record for <code>&lt;litellm-hostname&gt;</code> (check prerequisities section) that points to the ALB host name, then access LiteLLM using <code>&lt;litellm-hostname&gt;</code>.</p> </li> <li> <p>Verify LiteLLM through external endpoint</p> <pre><code>curl --location \"https://${LITELLM_HOSTNAME}/chat/completions\" \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer sk-1234' \\\n    --data '{\n    \"model\": \"bedrock-llama3-8b-instruct-v1\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"what llm are you\"\n        }\n    ]\n}'\n</code></pre> </li> </ol>"},{"location":"20-deploy/70-open-webui/","title":"(Optional) Connect Open WebUI to LiteLLM","text":"<p>Open WebUI is a web frontend that allows users to interact with LLMs. It supports locally running LLMs using Ollama, and OpenAI-compatible remote endpoints. In this implementation, we are configuring a remote endpoint that points to LiteLLM to show how LiteLLM allows for accessing Bedrock through an OpenAI-compatible interface. </p>"},{"location":"20-deploy/70-open-webui/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A domain that can be used for hosting Open WebUI, a web frontend that allows users to interact with LLMs; it will be used to test LiteLLM setup.</li> <li>A digital certificate in AWS Certificate Manager (ACM) for enabling TLS on Open WebUI</li> </ul>"},{"location":"20-deploy/70-open-webui/#open-webui-deployment","title":"Open WebUI deployment","text":"<ol> <li> <p>Configure environment variables; replace <code>&lt;open-webui-hostname&gt;</code>, <code>&lt;open-webui-cert-arn&gt;</code> with the corresponding hostnames and ACM certificates ARN.</p> <pre><code>export OPEN_WEBUI_HOSTNAME=\"&lt;open-webui-hostname&gt;\"\nexport OPEN_WEBUI_CERTIFICATE_ARN=\"&lt;open-webui-cert-arn&gt;\"\n\necho \"export OPEN_WEBUI_HOSTNAME=${OPEN_WEBUI_HOSTNAME}\" | tee -a ~/.bash_profile\necho \"export OPEN_WEBUI_CERTIFICATE_ARN=${OPEN_WEBUI_CERTIFICATE_ARN}\" | tee -a ~/.bash_profile\n</code></pre> </li> <li> <p>Install Open WebUI:     <pre><code>helm repo add open-webui https://helm.openwebui.com/\nhelm repo update\n\nhelm upgrade \\\n    --install open-webui open-webui/open-webui \\\n    --namespace open-webui \\\n    --create-namespace \\\n    -f bedrock-litellm/helm/open-webui-private-values.yaml\n</code></pre></p> <p>The first user signing up will get admin access. So, initially, Open WebUI will be only accessible from within the cluster to securely create the first/admin user. Subsequent sign ups will be in pending state till they are approved by the admin user.</p> </li> <li> <p>Use <code>kubectl port-forward</code> to allow access to Open WebUI from the machine used for installation:     <pre><code>kubectl port-forward service/open-webui -n open-webui  8080:80\n</code></pre></p> <p>If you are using Cloud9, you can access Open WebUI by clicking \"Preview\" (top bar), then \"Preview Running Application\".</p> </li> <li> <p>Sign-up (remember, first signed up user get admin access), then go to User icon at top right, settings, admin settings, connections, then edit OpenAI API to be as follows:</p> <pre><code>http://litellm-service.default.svc.cluster.local:4000/v1\n</code></pre> <p>Click on \"Verify connection\" button to make sure connectivity is in-place, then save. You should be able to see three of the Bedrock models available in Open WebUI as depicted in the screenshot below:</p> <p></p> <p>Now, we have the admin user created, we can make Open WebUI accessible publicly.</p> </li> <li> <p>Update Open WebUI helm release to include <code>Ingress</code> object for exposing it:     <pre><code>envsubst &lt; $BEDROCK_LITELLM_DIR/helm/open-webui-public-values.yaml | helm upgrade \\\n    open-webui open-webui/open-webui \\\n    --namespace open-webui \\\n    -f -\n</code></pre></p> <p>Note</p> <p>ELB needs a minute or so to complete the target registration; if the URL above did not work for you, wait for a few seconds for the registration to get completed.</p> </li> <li> <p>Extract Open WebUI URL:     <pre><code>kubectl -n open-webui get ingress open-webui  -o jsonpath='{.status.loadBalancer.ingress[*].hostname}'\n</code></pre></p> </li> <li> <p>Add a CNAME record for <code>&lt;open-webui-hostname&gt;</code> (check prerequisities section) that points to the ALB host name, then access Open WebUI using <code>&lt;open-webui-hostname&gt;</code>.</p> </li> <li> <p>Edit <code>litellm/proxy_config.yaml</code>, update the IAM policy <code>litellm-bedrock-policy.json</code>, and enable access through the Bedrock console to add more Bedrock models on LiteLLM.</p> </li> </ol>"},{"location":"20-deploy/99-clean-up/","title":"Clean-up","text":"<ol> <li> <p>Uninstall Open WebUI: <pre><code>helm uninstall open-webui --namespace open-webui\n</code></pre></p> </li> <li> <p>Uninstall LiteLLM <pre><code>kubectl delete -f $BEDROCK_LITELLM_DIR/litellm/ingress.yaml\nkubectl delete -f $LITELLM_DIR/deploy/kubernetes/service.yaml\nkubectl delete -f $LITELLM_DIR/deploy/kubernetes/kub.yaml\nkubectl delete configmap litellm-config\neksctl delete iamserviceaccount \\\n    --name litellm-sa \\\n    --cluster $CLUSTER_NAME\naws iam delete-policy --policy-arn $LITELLM_BEDROCK_IAM_POLICY_ARN\n</code></pre></p> </li> <li> <p>Uninstall AWS LBC <pre><code>helm uninstall aws-load-balancer-controller --namespace kube-system\neksctl delete iamserviceaccount \\\n    --name aws-load-balancer-controller \\\n    --namespace=kube-system \\\n    --cluster $CLUSTER_NAME\naws iam delete-policy --policy-arn $AWS_LBC_IAM_POLICY_ARN\n</code></pre></p> </li> <li> <p>Uninstall EBS driver <pre><code>helm uninstall aws-ebs-csi-driver \\\n    --namespace kube-system\neksctl delete iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --cluster $CLUSTER_NAME\n</code></pre></p> </li> <li> <p>Delete cluster <pre><code>eksctl delete cluster --name $CLUSTER_NAME\n</code></pre></p> </li> <li> <p>Delete the CNAME DNS records and the ACM certiciates used for LiteLLM and Open WebUI</p> </li> </ol>"},{"location":"25-config/10-model-alias/","title":"Model aliases","text":"<p>Model aliases help you define a user-friendly name for a model or group of models, abstracting away the actual model name used by the model provider. LiteLLM provides the ability to use model aliases, which allow you to present a simplified or user-friendly model name to your end-users while invoking a different, more specific model name on the backend. This is useful when you want to abstract Bedrock model ids from clients or when dealing with multiple versions of models.</p> <p>For example, you might display the model name <code>claude-3</code> to the end-user while internally calling <code>bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0</code> on the backend.</p> <p>In the configuration file <code>proxy_config_model_alias.yaml</code>, the <code>model_name</code> parameter (e.g., <code>claude-3</code>) is the user-facing name, while the <code>litellm_params.model</code> parameter contains the actual backend model id (e.g., <code>bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0</code>) and any additional parameters needed for model configuration.</p>"},{"location":"25-config/10-model-alias/#sample-configuration","title":"Sample configuration","text":"<pre><code>model_list:\n  - model_name: claude-3\n    litellm_params:\n      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n      aws_region_name: $AWS_REGION\n</code></pre>"},{"location":"25-config/10-model-alias/#steps-to-configure","title":"Steps to configure","text":"<ol> <li>Use the configuration file <code>$BEDROCK_LITELLM_DIR/litellm/config/proxy_config_model_alias.yaml</code></li> <li>To apply the new configuration, follow the steps outlined in Apply configuration changes.</li> </ol>"},{"location":"25-config/10-model-alias/#steps-to-test","title":"Steps to test","text":"<ol> <li>Test the model alias by invoking the user-facing model name as shown <pre><code>curl --location \"https://${LITELLM_HOSTNAME}/chat/completions\" \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer &lt;your key&gt;' \\\n  --data '{\n    \"model\": \"claude-3\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"what is Amazon S3\"\n      }\n    ]\n  }'\n</code></pre></li> </ol>"},{"location":"25-config/20-rate-limit/","title":"Rate limiting","text":"<p>LiteLLM provides rate-limiting capabilities to control the number of requests or tokens that can be processed by a model over a specific time. This feature allows you to manage traffic, control costs, and ensure fairness by restricting access based on requests per minute (RPM) or tokens per minute (TPM).</p> <p>Rate limits can be applied:</p> <ol> <li>Per API key</li> <li>Per user</li> <li>Per team</li> <li>Per specific models</li> </ol> <p>You can specify both rpm_limit (requests per minute) and tpm_limit (tokens per minute) for models, users, or teams in your configuration.</p>"},{"location":"25-config/20-rate-limit/#steps-to-configure","title":"Steps to configure","text":"<ol> <li>Create user with RPM and TPM values. You can create a user and define the limits for RPM and TPM by sending a request to the user creation API. This will ensure that the user is rate-limited accordingly.   <pre><code>curl --location 'http://${LITELLM_HOSTNAME}/user/new' \\\n--header 'Authorization: Bearer &lt;your key&gt;' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"user_id\": \"test_user_1\", \n    \"max_parallel_requests\": 10, \n    \"tpm_limit\": 20, \n    \"rpm_limit\": 2\n}'\n</code></pre>   You should get a <code>key</code> in the response header. This will serve as a master key while making chat request. For example, lets say value returned is <code>sk-1234567</code>.</li> </ol>"},{"location":"25-config/20-rate-limit/#steps-to-test","title":"Steps to test","text":"<ol> <li>Using the master key obtained in the previous step, you can make a request to the chat API. Ensure the correct model and message are passed, and the authorization header includes the master key:   <pre><code>curl --location \"http://${LITELLM_HOSTNAME}/chat/completions\" \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer sk-1234567' \\\n    --data '{\n    \"model\": \"claude-3\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"what llm are you\"\n        }\n    ]\n}'\n</code></pre></li> <li>After making more than 2 requests, you should start receiving an error response indicating that the RPM limit has been reached. Here is an example of the error response you might see:   <pre><code>{\n    \"error\": {\n        \"message\": \"Max parallel request limit reached. Hit limit for api_key: xxx. tpm_limit: 20, current_tpm 46, rpm_limit: 2, current rpm 1\",\n        \"type\": \"None\",\n        \"param\": \"None\",\n        \"code\": \"429\"\n    }\n}\n</code></pre>   Similarly you can perform other tests on TPM as well.</li> </ol> <p>Please note the above test is for applying rate limit on user level. If you want to test different configurations on teams, organization etc, you can follow LiteLLM documentation here.</p>"},{"location":"25-config/25-global-rate-limit/","title":"Global rate limiting","text":"<p>LiteLLM allows you to apply requests per minute (RPM) and tokens per minute (TPM) limits globally across all users, teams, and models through the LiteLLM configuration file. These global limits ensure that traffic is controlled across all requests, regardless of individual user or team limits.</p> <p>Note</p> <p>You do not need to configure a database URL or use the LLM master key to apply this configuration, making it simpler for deployments where per-user tracking is not required.</p>"},{"location":"25-config/25-global-rate-limit/#sample-configuration","title":"Sample configuration","text":"<pre><code>model_list:\n  - model_name: claude-3\n    litellm_params:\n      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n      aws_region_name: $AWS_REGION\n      rpm: 2 \n      tpm: 200\n\nrouter_settings:\n  enable_pre_call_checks: true # 1. Enable pre-call checks\n</code></pre>"},{"location":"25-config/25-global-rate-limit/#steps-to-configure","title":"Steps to configure","text":"<ol> <li>Use the configuration file <code>$BEDROCK_LITELLM_DIR/litellm/config/proxy_config_global_rate_limit.yaml</code></li> <li>To apply the new configuration, follow the steps outlined in Apply configuration changes.</li> </ol>"},{"location":"25-config/25-global-rate-limit/#steps-to-test","title":"Steps to test","text":"<ol> <li>To test the global rate limit, make three or more API requests within one minute. After the second request, you should start receiving an error indicating that the limit has been reached. The limit will reset after one minute.   <pre><code>curl --location \"http://${LITELLM_HOSTNAME}/chat/completions\" \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"claude-3\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"what is amazon S3\"\n      }\n    ]\n  }'\n</code></pre>   If the rate limit is exceeded, you should receive an error response similar to the one below:   <pre><code>{\n  \"error\":\n    {\n      \"message\":\"No deployments available for selected model, Try again in 60 seconds. Passed model=bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0. Try again in 60 seconds.\",\n      \"type\":\"None\",\n      \"param\":\"None\",\n      \"code\":\"429\"\n    }\n}\n</code></pre></li> </ol>"},{"location":"25-config/30-route/","title":"Routing","text":"<p>LiteLLM's routing feature allows for dynamic control over how requests are directed to LLMs deployed across multiple backends. The routing configuration is critical for optimizing load distribution, cost management, fallback strategies, and latency.</p>"},{"location":"25-config/30-route/#steps-to-configure","title":"Steps to configure","text":"<p>To apply different routing configurations (such as load balancing, fallback, rate limit-aware routing, or latency-based routing), follow these steps:</p> <ol> <li>Use one of the configuration files at <code>$BEDROCK_LITELLM_DIR/litellm/config/route/</code></li> <li>Follow the steps outlined in Apply configuration changes.</li> </ol>"},{"location":"25-config/30-route/#steps-to-test","title":"Steps to test","text":"<ol> <li>Test by making a call to the chat API: <pre><code>curl --location \"https://${LITELLM_HOSTNAME}/chat/completions\" \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer &lt;your key&gt;' \\\n  --data '{\n    \"model\": \"claude-3\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"what is Amazon S3\"\n      }\n    ]\n  }'\n</code></pre></li> </ol>"},{"location":"25-config/30-route/#sample-configurations","title":"Sample configurations","text":"<p>The sample configurations below demonstrate LiteLLM key routing functionalities.</p>"},{"location":"25-config/30-route/#load-balancing","title":"Load balancing","text":"<p>LiteLLM distributes requests across multiple model instances using various strategies such as round-robin(default), least busy etc.</p> <pre><code>model_list:\n  - model_name: claude-3\n    litellm_params:\n      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n      aws_region_name: $AWS_REGION\n\n  - model_name: claude-3\n    litellm_params:\n      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0\n      aws_region_name: $AWS_REGION\n</code></pre>"},{"location":"25-config/30-route/#fallbacks","title":"Fallbacks","text":"<p>In case of a failure from a primary model, LiteLLM automatically redirects the request to a fallback model. This ensures uninterrupted service even if a model or provider is down.</p> <pre><code>model_list:\n  - model_name: claude-3-sonnet\n    litellm_params:\n      model: bedrock/invalid\n      aws_region_name: $AWS_REGION\n\n  - model_name: claude-3-haiku\n    litellm_params:\n      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0\n      aws_region_name: $AWS_REGION\n\nrouter_settings:\n    enable_pre_call_checks: true # 1. Enable pre-call checks\n\nlitellm_settings:\n  num_retries: 3 # retry call 3 times on each model_name (e.g. zephyr-beta)\n  request_timeout: 10 # raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout \n  fallbacks: [{\"claude-3-sonnet\": [\"claude-3-haiku\"]}] \n  allowed_fails: 3 # cooldown model if it fails &gt; 1 call in a minute. \n  cooldown_time: 30 # how long to cooldown model if fails/min &gt; allowed_fails\n</code></pre> <p>For testing, make sure when you pass additional field called <code>mock_testing_fallbacks</code> as shown below: <pre><code>curl --location \"http://${LITELLM_HOSTNAME}/chat/completions\" \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer &lt;your key&gt;' \\\n    --data '{\n    \"model\": \"claude-3-sonnet\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"what llm are you\"\n        }\n    ], \"mock_testing_fallbacks\": true\n}'\n</code></pre></p>"},{"location":"25-config/30-route/#rate-limit-aware-routing","title":"Rate limit-aware routing","text":"<p>LiteLLM can dynamically reroute requests if a model has exceeded its rate limit (requests per minute or tokens per minute). This prevents service disruption when models reach their capacity limits.</p> <pre><code>model_list:\n  - model_name: claude-3-sonnet\n    litellm_params: \n      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n      aws_region_name: $AWS_REGION\n    tpm: 2000\n    rpm: 10\n  - model_name: claude-3-haiku\n    litellm_params: \n      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0\n      aws_region_name: $AWS_REGION\n    tpm: 10000\n    rpm: 1\n\nrouter_settings:\n  routing_strategy: usage-based-routing-v2\n  enable_pre_call_check: true\n</code></pre> <p>In this configuration, Claude Sonnet is limited to 10 requests per minute and 2000 tokens per minute. When exceeding these limits, LiteLLM filters out the deployment, and routes to the deployment with the lowest TPM usage for that minute.</p>"},{"location":"25-config/30-route/#latency-based-routing","title":"Latency-based routing","text":"<p>LiteLLM can prioritize routing based on model response times (latency). It Picks the deployment with the lowest response time by caching, and updating the response times for deployments based on when a request was sent and received from a deployment.</p> <pre><code>model_list:\n  - model_name: claude-3\n    litellm_params: \n      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n      aws_region_name: $AWS_REGION\n\n  - model_name: claude-3\n    litellm_params: \n      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0\n      aws_region_name: $AWS_REGION\n\nrouter_settings:\n  routing_strategy: latency-based-routing\"\n  enable_pre_call_check: true\n</code></pre> <p>For more details on routing, please refer to the LiteLLM docs.</p>"},{"location":"25-config/40-apply-config-changes/","title":"Apply configuration changes","text":"<p>In order to apply LiteLLM configuration changes in ConfigMap, follow the steps below to ensure the deployment is patched, thereby forcing a pod restart to reflect latest changes:</p> <p>Note</p> <p>If  the configuration file contains environment variables (e.g., <code>$AWS_REGION</code>), use the <code>envsubst</code> command to replace the variables with their actual values, before updating the ConfigMap. See an example below.</p> <p>Note</p> <p>Replace the filename below to reflect your file with configuration update i.e. <code>$BEDROCK_LITELLM_DIR/litellm/config/proxy_config_model_alias.yaml</code> to be replaced with your relevant file that contains the updated configuration.</p> <ol> <li>Run the following command to replace the environment variables with their values. <pre><code>envsubst &lt; $BEDROCK_LITELLM_DIR/litellm/config/proxy_config_model_alias.yaml &gt; /tmp/proxy_config_model_alias.yaml\n</code></pre></li> <li>Run the following command to update the configMap with latest configuration from a file. <pre><code>kubectl create configmap litellm-config --from-file=litellm-config.yaml=/tmp/proxy_config_model_alias.yaml --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></li> <li>Patch the deployment to reflect the changes. After updating the ConfigMap, the pods won\u2019t automatically reload the configuration. You need to patch the deployment to force a pod restart, ensuring that the new configuration is picked up. <pre><code>kubectl patch deployment litellm-deployment -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"configmap-update-timestamp\\\":\\\"$(date +'%s')\\\"}}}}}\"\n</code></pre></li> <li>Once the pods are restarted, verify the new configuration is applied by checking the logs.</li> </ol>"},{"location":"30-app-changes/10-app-changes/","title":"Code Changes for OpenAI to Amazon Bedrock Migration","text":"<p>With LiteLLM successfully deployed onto Amazon EKS and proxying requests to Amazon Bedrock, you can choose to migrate from OpenAI with minimal code changes.</p> <p>Requests from your applications that do not originate from Open WebUI can be modified by updating your OpenAI base endpoint to point to your ALB DNS name. This is similar to the change we made in step 17, updating an OpenAI endpoint to point to your LiteLLM service, this time to the ALB host name, or your CNAME record for  (check prerequisities section) that points to the ALB host name. <ol> <li> <p>Update your application's OpenAI API endpoint to point to your . <pre><code>import openai\n\nopenai.api_base = {\"your-litellm-hostname\"}\nopenai.api_key = {\"your-open-ai-api-key\"}\n\n# Your existing OpenAI code remains unchanged\nresponse = openai.Completion.create(\nmodel=\"text-davinci-003\",\nprompt=\"Translate the following English text to French: 'Hello, how are you?'\"\n)\n</code></pre> <li> <p>Test and validate that your existing code and application work as expected, calling foundation models hosted on Amazon Bedrock via LiteLLM hosted on Amazon EKS. Best practices and considerations:</p> <ol> <li>Gradually migrate: Start by routing a small percentage of traffic through the LiteLLM proxy and gradually increase as you gain confidence.</li> <li>Monitor performance: Use Amazon CloudWatch to monitor the performance and AWS Cost Explorer to monitor the costs of your AWS usage, including Amazon Bedrock.</li> <li>Security: Ensure least privilege AWS Identity and Access Management (AWS IAM) roles and security groups are in place for your EKS cluster and Amazon Bedrock access.</li> <li>Scalability: Configure auto-scaling for your EKS nodes to handle varying loads.</li> </ol> </li>"}]}